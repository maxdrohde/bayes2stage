<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>Bayesian Two-Stage Designs for Longitudinal Studies with Costly Covariates ‚Ä¢ bayes2stage</title><script src="deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><link href="deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet"><script src="deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="deps/font-awesome-6.5.2/css/all.min.css" rel="stylesheet"><link href="deps/font-awesome-6.5.2/css/v4-shims.min.css" rel="stylesheet"><script src="deps/headroom-0.11.0/headroom.min.js"></script><script src="deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="deps/search-1.0.0/fuse.min.js"></script><script src="deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="pkgdown.js"></script><meta property="og:title" content="Bayesian Two-Stage Designs for Longitudinal Studies with Costly Covariates"></head><body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top bg-light" data-bs-theme="light" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="index.html">bayes2stage</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">0.0.0.9000</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto"><li class="nav-item"><a class="nav-link" href="articles/bayes2stage.html">Get started</a></li>
<li class="nav-item"><a class="nav-link" href="reference/index.html">Reference</a></li>
<li class="nav-item"><a class="nav-link" href="news/index.html">Changelog</a></li>
      </ul><ul class="navbar-nav"><li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="search.json"></form></li>
<li class="nav-item"><a class="external-link nav-link" href="https://github.com/maxdrohde/bayes2stage/" aria-label="GitHub"><span class="fa fab fa-github fa-lg"></span></a></li>
      </ul></div>


  </div>
</nav><div class="container template-title-body">
<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">

      <h1>Bayesian Two-Stage Designs for Longitudinal Studies with Costly Covariates</h1>
      <small class="dont-index">Source: <a href="https://github.com/maxdrohde/bayes2stage/blob/master/paper_draft.md" class="external-link"><code>paper_draft.md</code></a></small>
    </div>

<div id="bayesian-two-stage-designs-for-longitudinal-studies-with-costly-covariates" class="section level1">

<p><strong>Authors</strong>: [Author names]</p>
<p><strong>Affiliations</strong>: [Institutions]</p>
<p><strong>Correspondence</strong>: [Email]</p>
<hr><div class="section level2">
<h2 id="abstract">Abstract<a class="anchor" aria-label="anchor" href="#abstract"></a></h2>
<p>Longitudinal studies frequently aim to understand how expensive-to-measure covariates‚Äîbiomarkers, genetic profiles, or detailed environmental exposures‚Äîshape health trajectories over time. When budget constraints preclude measuring every subject, researchers must decide which individuals to assess, ideally selecting those who will contribute the most statistical information.</p>
<p>We develop a Bayesian two-stage framework that addresses this challenge. In Stage 1, inexpensive longitudinal outcomes are collected on all subjects. In Stage 2, expensive covariates are measured on a strategically selected subset, guided by Stage 1 trajectory information. Our framework supports three selection strategies: simple random sampling (SRS), outcome-dependent sampling (ODS) based on ordinary least squares trajectory estimates, and BLUP-dependent sampling (BDS) that leverages mixed model predictions to identify informative subjects more accurately.</p>
<p>For estimation, we introduce a joint modeling approach that combines mixed-effects outcome models with flexible covariate imputation, accommodating continuous, binary, and count covariates. Our optimized Stan implementation marginalizes over discrete missing covariates analytically, avoiding the mixing problems that plague discrete parameter sampling. We also provide a frequentist ascertainment-corrected maximum likelihood (ACML) estimator for comparison.</p>
<p>Extensive simulations demonstrate that tail-focused ODS and BDS designs substantially improve precision for covariate effects relative to random sampling, with BDS offering additional gains when trajectory estimates are unreliable. Coverage remains well-calibrated under correctly specified imputation models. All methods are implemented in the open-source R package <code>bayes2stage</code>.</p>
<p><strong>Keywords</strong>: two-stage design, outcome-dependent sampling, longitudinal data, Bayesian imputation, mixed effects models, BLUP, Stan</p>
<hr></div>
<div class="section level2">
<h2 id="id_1-introduction">1. Introduction<a class="anchor" aria-label="anchor" href="#id_1-introduction"></a></h2>
<p>The promise of precision medicine and personalized health research rests on understanding how individual characteristics‚Äîgenetic variants, biomarker profiles, environmental exposures‚Äîinfluence disease trajectories. Yet the covariates that matter most are often the most expensive to measure. A novel proteomic panel might cost hundreds of dollars per assay; whole-genome sequencing remains a significant expense; comprehensive environmental monitoring requires specialized equipment and labor-intensive protocols. Meanwhile, health outcomes flow readily from electronic records, routine clinical visits, or inexpensive questionnaires.</p>
<p>This asymmetry between the cost of covariates and outcomes creates a design challenge. Researchers with cohorts of thousands cannot afford to measure expensive covariates on everyone, but they recognize that measuring only a handful of subjects will yield underpowered analyses. The question becomes: given a fixed budget, which subjects should receive expensive measurements to maximize what we learn about covariate-outcome relationships?</p>
<p>The naive answer‚Äîselect subjects at random‚Äîis statistically valid but inefficient. Not all subjects carry equal information about how a covariate influences trajectories. Consider a study of how an inflammatory biomarker affects cognitive decline. A subject whose cognition remains rock-steady over years tells us something different than one who declines precipitously. If the biomarker truly matters, these extreme responders are more likely to have informative biomarker values‚Äîeither very high or very low. Randomly sampling subjects ignores this structure, wasting measurement resources on individuals whose middling trajectories reveal little about the biomarker‚Äôs effects.</p>
<p>Two-stage designs formalize the intuition that some subjects are more informative than others. In Stage 1, inexpensive longitudinal outcomes are collected on everyone, revealing individual trajectory patterns. In Stage 2, expensive covariates are measured on a subset selected to be maximally informative‚Äîoften by oversampling subjects with extreme trajectories. The resulting data are then analyzed using methods that properly account for the selective sampling.</p>
<p>This paper develops a comprehensive Bayesian framework for two-stage longitudinal designs. We make three primary contributions. First, we introduce BLUP-dependent sampling (BDS), which uses mixed model predictions rather than raw ordinary least squares estimates to identify informative subjects. By leveraging shrinkage and borrowing strength across subjects, BDS more accurately identifies truly extreme individuals, particularly when trajectory estimates are noisy. Second, we develop a joint Bayesian model that simultaneously estimates outcome-covariate relationships and imputes missing covariate values, with optimized implementations for continuous, binary, and count covariates. Third, we provide extensive simulation evidence comparing sampling strategies and estimation approaches, offering practical guidance for applied researchers.</p>
<p>The methods are implemented in the R package <code>bayes2stage</code>, which provides design functions for constructing Stage 2 samples, Stan-based Bayesian estimation, and a frequentist ACML estimator for comparison. Our goal is to make these sophisticated methods accessible to researchers facing the ubiquitous challenge of expensive covariates in longitudinal research.</p>
<hr></div>
<div class="section level2">
<h2 id="id_2-methods">2. Methods<a class="anchor" aria-label="anchor" href="#id_2-methods"></a></h2>
<div class="section level3">
<h3 id="id_21-data-structure-and-outcome-model">2.1 Data Structure and Outcome Model<a class="anchor" aria-label="anchor" href="#id_21-data-structure-and-outcome-model"></a></h3>
<p>We consider a longitudinal study with <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math> subjects. Subject <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math> contributes <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>M</mi><mi>i</mi></msub><annotation encoding="application/x-tex">M_i</annotation></semantics></math> outcome measurements <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>y</mi><mrow><mi>i</mi><mn>1</mn></mrow></msub><mo>,</mo><mi>‚Ä¶</mi><mo>,</mo><msub><mi>y</mi><mrow><mi>i</mi><msub><mi>M</mi><mi>i</mi></msub></mrow></msub></mrow><annotation encoding="application/x-tex">y_{i1}, \ldots, y_{iM_i}</annotation></semantics></math> collected at times <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>t</mi><mrow><mi>i</mi><mn>1</mn></mrow></msub><mo>,</mo><mi>‚Ä¶</mi><mo>,</mo><msub><mi>t</mi><mrow><mi>i</mi><msub><mi>M</mi><mi>i</mi></msub></mrow></msub></mrow><annotation encoding="application/x-tex">t_{i1}, \ldots, t_{iM_i}</annotation></semantics></math>. An expensive covariate <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>x</mi><mi>i</mi></msub><annotation encoding="application/x-tex">x_i</annotation></semantics></math>, measured only for a subset of subjects, is the primary predictor of interest. An inexpensive auxiliary covariate <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>z</mi><mi>i</mi></msub><annotation encoding="application/x-tex">z_i</annotation></semantics></math>, available for everyone, may predict both the outcome and the expensive covariate.</p>
<p>We model outcomes using a linear mixed effects specification:</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>y</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><mi>Œ±</mi><mo>+</mo><msub><mi>Œ≤</mi><mi>x</mi></msub><msub><mi>x</mi><mi>i</mi></msub><mo>+</mo><msub><mi>Œ≤</mi><mi>z</mi></msub><msub><mi>z</mi><mi>i</mi></msub><mo>+</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>Œ≤</mi><mi>t</mi></msub><mo>+</mo><msub><mi>b</mi><mrow><mn>1</mn><mi>i</mi></mrow></msub><mo stretchy="true" form="postfix">)</mo></mrow><msub><mi>t</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>+</mo><msub><mi>Œ≤</mi><mrow><mi>t</mi><mi>x</mi></mrow></msub><msub><mi>x</mi><mi>i</mi></msub><msub><mi>t</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>+</mo><msub><mi>b</mi><mrow><mn>0</mn><mi>i</mi></mrow></msub><mo>+</mo><msub><mi>œµ</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow><annotation encoding="application/x-tex">y_{ij} = \alpha + \beta_x x_i + \beta_z z_i + (\beta_t + b_{1i}) t_{ij} + \beta_{tx} x_i t_{ij} + b_{0i} + \epsilon_{ij}</annotation></semantics></math></p>
<p>The fixed effects decompose as follows. The intercept <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Œ±</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics></math> represents the population-average outcome at baseline for subjects with <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>=</mo><msub><mi>z</mi><mi>i</mi></msub><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">x_i = z_i = 0</annotation></semantics></math>. The coefficient <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>Œ≤</mi><mi>x</mi></msub><annotation encoding="application/x-tex">\beta_x</annotation></semantics></math> captures the main effect of the expensive covariate on outcome level‚Äîthe central parameter in many applications. The coefficient <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>Œ≤</mi><mi>z</mi></msub><annotation encoding="application/x-tex">\beta_z</annotation></semantics></math> adjusts for the auxiliary covariate. The time slope <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>Œ≤</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\beta_t</annotation></semantics></math> represents population-average change over time, and the interaction <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>Œ≤</mi><mrow><mi>t</mi><mi>x</mi></mrow></msub><annotation encoding="application/x-tex">\beta_{tx}</annotation></semantics></math> allows the expensive covariate to modify this trajectory. When <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Œ≤</mi><mrow><mi>t</mi><mi>x</mi></mrow></msub><mo>‚â†</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\beta_{tx} \neq 0</annotation></semantics></math>, the covariate influences not just where subjects start but how quickly they change‚Äîoften the scientifically most interesting question.</p>
<p>Subject-specific deviations from population averages are captured by correlated random effects:</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mo stretchy="true" form="prefix">(</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><msub><mi>b</mi><mrow><mn>0</mn><mi>i</mi></mrow></msub></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><msub><mi>b</mi><mrow><mn>1</mn><mi>i</mi></mrow></msub></mtd></mtr></mtable><mo stretchy="true" form="postfix">)</mo></mrow><mo>‚àº</mo><mi>ùí©</mi><mrow><mo stretchy="true" form="prefix">(</mo><mrow><mo stretchy="true" form="prefix">(</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd></mtr></mtable><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo><mi>Œ£</mi><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><msubsup><mi>œÉ</mi><mrow><mi>b</mi><mn>0</mn></mrow><mn>2</mn></msubsup></mtd><mtd columnalign="center" style="text-align: center"><mi>œÅ</mi><msub><mi>œÉ</mi><mrow><mi>b</mi><mn>0</mn></mrow></msub><msub><mi>œÉ</mi><mrow><mi>b</mi><mn>1</mn></mrow></msub></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mi>œÅ</mi><msub><mi>œÉ</mi><mrow><mi>b</mi><mn>0</mn></mrow></msub><msub><mi>œÉ</mi><mrow><mi>b</mi><mn>1</mn></mrow></msub></mtd><mtd columnalign="center" style="text-align: center"><msubsup><mi>œÉ</mi><mrow><mi>b</mi><mn>1</mn></mrow><mn>2</mn></msubsup></mtd></mtr></mtable><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\begin{pmatrix} b_{0i} \\ b_{1i} \end{pmatrix} \sim \mathcal{N}\left(\begin{pmatrix} 0 \\ 0 \end{pmatrix}, \Sigma = \begin{pmatrix} \sigma_{b0}^2 &amp; \rho\sigma_{b0}\sigma_{b1} \\ \rho\sigma_{b0}\sigma_{b1} &amp; \sigma_{b1}^2 \end{pmatrix}\right)</annotation></semantics></math></p>
<p>The random intercept <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>b</mi><mrow><mn>0</mn><mi>i</mi></mrow></msub><annotation encoding="application/x-tex">b_{0i}</annotation></semantics></math> shifts subject <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>‚Äôs entire trajectory up or down; the random slope <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>b</mi><mrow><mn>1</mn><mi>i</mi></mrow></msub><annotation encoding="application/x-tex">b_{1i}</annotation></semantics></math> tilts the trajectory steeper or flatter. Their correlation <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>œÅ</mi><annotation encoding="application/x-tex">\rho</annotation></semantics></math> allows subjects who start high to also change faster (or slower). Residual errors <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>œµ</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>‚àº</mo><mi>ùí©</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>0</mn><mo>,</mo><msubsup><mi>œÉ</mi><mtext mathvariant="normal">main</mtext><mn>2</mn></msubsup><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\epsilon_{ij} \sim \mathcal{N}(0, \sigma^2_{\text{main}})</annotation></semantics></math> capture measurement occasion variability.</p>
<p>Throughout, we scale time to the unit interval to improve numerical stability and facilitate prior specification.</p>
</div>
<div class="section level3">
<h3 id="id_22-stage-2-sampling-designs">2.2 Stage 2 Sampling Designs<a class="anchor" aria-label="anchor" href="#id_22-stage-2-sampling-designs"></a></h3>
<p>The central design question is which subjects should have their expensive covariate measured. We consider three strategies, implemented in dedicated package functions.</p>
<p><strong>Simple Random Sampling (SRS)</strong> selects subjects with equal probability, implemented via <code><a href="reference/srs_design.html">srs_design()</a></code>. This approach ignores Stage 1 outcome data entirely, treating all subjects as equally informative. While valid, SRS serves primarily as a baseline against which to compare more sophisticated strategies.</p>
<p><strong>Outcome-Dependent Sampling (ODS)</strong> exploits Stage 1 trajectories to identify informative subjects. The <code><a href="reference/ods_design.html">ods_design()</a></code> function first estimates subject-specific intercepts and slopes by fitting separate ordinary least squares regressions to each individual‚Äôs data. Subjects are then stratified into groups‚Äîtypically three, labeled Low, Middle, and High‚Äîbased on quantiles of these estimates. Sampling overweights the tails: a common allocation devotes 40% of measurements to the Low stratum, 20% to Middle, and 40% to High.</p>
<p>The logic is straightforward. If the expensive covariate influences outcome levels, subjects with extreme observed intercepts (after accounting for <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>z</mi><annotation encoding="application/x-tex">z</annotation></semantics></math>) likely have extreme covariate values. Oversampling these subjects concentrates measurement resources where they yield the most information about <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>Œ≤</mi><mi>x</mi></msub><annotation encoding="application/x-tex">\beta_x</annotation></semantics></math>. Stratification can alternatively target slopes when the interaction <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>Œ≤</mi><mrow><mi>t</mi><mi>x</mi></mrow></msub><annotation encoding="application/x-tex">\beta_{tx}</annotation></semantics></math> is of primary interest.</p>
<p><strong>BLUP-Dependent Sampling (BDS)</strong> refines ODS by using Best Linear Unbiased Predictors from a mixed effects model fitted to Stage 1 data, implemented via <code><a href="reference/bds_design.html">bds_design()</a></code>. The key insight is that OLS estimates treat all subjects equally, even when some contribute few observations or highly variable measurements. A subject with two noisy data points receives an extreme OLS slope that may reflect measurement error rather than a truly unusual trajectory. BLUPs, by contrast, shrink unreliable estimates toward the population mean, reserving extreme predictions for subjects whose data genuinely support them.</p>
<p>BDS is most valuable when trajectory estimates are unreliable‚Äîwhen subjects have few time points, measurement error is substantial, or between-subject heterogeneity is large. When trajectories are precisely estimated, BDS and ODS converge, and the simpler OLS-based approach suffices.</p>
</div>
<div class="section level3">
<h3 id="id_23-bayesian-joint-model-and-imputation">2.3 Bayesian Joint Model and Imputation<a class="anchor" aria-label="anchor" href="#id_23-bayesian-joint-model-and-imputation"></a></h3>
<p>With Stage 2 subjects selected and their expensive covariates measured, we turn to estimation. The challenge is that <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>x</mi><mi>i</mi></msub><annotation encoding="application/x-tex">x_i</annotation></semantics></math> is missing for unselected subjects. Rather than discarding these individuals, we develop a joint model that leverages all available data.</p>
<p><strong>Model Structure.</strong> We specify a joint distribution factoring into an outcome component and an imputation component:</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo>,</mo><msub><mi>x</mi><mi>i</mi></msub><mo>‚à£</mo><msub><mi>z</mi><mi>i</mi></msub><mo>,</mo><mi>Œ∏</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>p</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo>‚à£</mo><msub><mi>x</mi><mi>i</mi></msub><mo>,</mo><msub><mi>z</mi><mi>i</mi></msub><mo>,</mo><msub><mi>Œ∏</mi><mi>y</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>√ó</mo><mi>p</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>‚à£</mo><msub><mi>z</mi><mi>i</mi></msub><mo>,</mo><msub><mi>Œ∏</mi><mi>x</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">p(y_i, x_i \mid z_i, \theta) = p(y_i \mid x_i, z_i, \theta_y) \times p(x_i \mid z_i, \theta_x)</annotation></semantics></math></p>
<p>The outcome component is the mixed effects model described above. The imputation component links the expensive covariate to the auxiliary covariate, providing a mechanism for predicting <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math> when it is unobserved.</p>
<p>For subjects with observed <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math>, both components contribute directly to the likelihood. For subjects with missing <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math>, we marginalize:</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo>‚à£</mo><msub><mi>z</mi><mi>i</mi></msub><mo>,</mo><mi>Œ∏</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mo>‚à´</mo><mi>p</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo>‚à£</mo><msub><mi>x</mi><mi>i</mi></msub><mo>,</mo><msub><mi>z</mi><mi>i</mi></msub><mo>,</mo><msub><mi>Œ∏</mi><mi>y</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>√ó</mo><mi>p</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>‚à£</mo><msub><mi>z</mi><mi>i</mi></msub><mo>,</mo><msub><mi>Œ∏</mi><mi>x</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mspace width="0.167em"></mspace><mi>d</mi><msub><mi>x</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">p(y_i \mid z_i, \theta) = \int p(y_i \mid x_i, z_i, \theta_y) \times p(x_i \mid z_i, \theta_x) \, dx_i</annotation></semantics></math></p>
<p>This integral averages over possible covariate values, weighted by their probability under the imputation model. The outcome data from unselected subjects still contribute to inference‚Äîthey inform random effects variance components and, through the imputation model, the covariate distribution.</p>
<p><strong>Imputation Model Specifications.</strong> The package supports four covariate types, each with an appropriate imputation distribution:</p>
<p>For <em>continuous covariates</em> (e.g., biomarker concentrations), we specify a normal linear regression: <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>‚à£</mo><msub><mi>z</mi><mi>i</mi></msub><mo>‚àº</mo><mi>ùí©</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>Œ±</mi><mtext mathvariant="normal">imp</mtext></msub><mo>+</mo><mi>Œ≥</mi><msub><mi>z</mi><mi>i</mi></msub><mo>,</mo><msubsup><mi>œÉ</mi><mtext mathvariant="normal">imp</mtext><mn>2</mn></msubsup><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">x_i \mid z_i \sim \mathcal{N}(\alpha_{\text{imp}} + \gamma z_i, \sigma^2_{\text{imp}})</annotation></semantics></math></p>
<p>For <em>binary covariates</em> (e.g., presence of a genetic variant), we use logistic regression: <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>‚à£</mo><msub><mi>z</mi><mi>i</mi></msub><mo>‚àº</mo><mtext mathvariant="normal">Bernoulli</mtext><mrow><mo stretchy="true" form="prefix">(</mo><msup><mtext mathvariant="normal">logit</mtext><mrow><mo>‚àí</mo><mn>1</mn></mrow></msup><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>Œ±</mi><mtext mathvariant="normal">imp</mtext></msub><mo>+</mo><mi>Œ≥</mi><msub><mi>z</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">x_i \mid z_i \sim \text{Bernoulli}\left(\text{logit}^{-1}(\alpha_{\text{imp}} + \gamma z_i)\right)</annotation></semantics></math></p>
<p>For <em>unbounded count covariates</em> (e.g., number of risk alleles), we employ a negative binomial model: <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>‚à£</mo><msub><mi>z</mi><mi>i</mi></msub><mo>‚àº</mo><mtext mathvariant="normal">NegBinom</mtext><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>Œº</mi><mi>i</mi></msub><mo>=</mo><mo>exp</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>Œ±</mi><mtext mathvariant="normal">imp</mtext></msub><mo>+</mo><mi>Œ≥</mi><msub><mi>z</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo><mi>œï</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">x_i \mid z_i \sim \text{NegBinom}\left(\mu_i = \exp(\alpha_{\text{imp}} + \gamma z_i), \phi\right)</annotation></semantics></math> where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>œï</mi><annotation encoding="application/x-tex">\phi</annotation></semantics></math> governs overdispersion.</p>
<p>For <em>bounded count covariates</em> (e.g., scores with a maximum), we use a beta-binomial: <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>‚à£</mo><msub><mi>z</mi><mi>i</mi></msub><mo>‚àº</mo><mtext mathvariant="normal">BetaBinom</mtext><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>n</mi><mtext mathvariant="normal">trials</mtext></msub><mo>,</mo><msub><mi>Œ±</mi><mrow><mi>b</mi><mi>b</mi></mrow></msub><mo>,</mo><msub><mi>Œ≤</mi><mrow><mi>b</mi><mi>b</mi></mrow></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">x_i \mid z_i \sim \text{BetaBinom}(n_{\text{trials}}, \alpha_{bb}, \beta_{bb})</annotation></semantics></math> where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>Œ±</mi><mrow><mi>b</mi><mi>b</mi></mrow></msub><annotation encoding="application/x-tex">\alpha_{bb}</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>Œ≤</mi><mrow><mi>b</mi><mi>b</mi></mrow></msub><annotation encoding="application/x-tex">\beta_{bb}</annotation></semantics></math> are functions of a mean parameter and concentration <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>œï</mi><annotation encoding="application/x-tex">\phi</annotation></semantics></math>.</p>
<p><strong>Handling Missing Covariates.</strong> The marginalization integral takes different forms depending on covariate type. For continuous <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math>, we either exploit conjugacy or sample latent values. For discrete <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math>, we marginalize analytically by summing over possible values:</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo>‚à£</mo><msub><mi>z</mi><mi>i</mi></msub><mo>,</mo><mi>Œ∏</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><munderover><mo>‚àë</mo><mrow><mi>x</mi><mo>=</mo><mn>0</mn></mrow><msub><mi>x</mi><mo>max</mo></msub></munderover><mi>p</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo>‚à£</mo><msub><mi>x</mi><mi>i</mi></msub><mo>=</mo><mi>x</mi><mo>,</mo><msub><mi>z</mi><mi>i</mi></msub><mo>,</mo><msub><mi>Œ∏</mi><mi>y</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>√ó</mo><mi>p</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>=</mo><mi>x</mi><mo>‚à£</mo><msub><mi>z</mi><mi>i</mi></msub><mo>,</mo><msub><mi>Œ∏</mi><mi>x</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">p(y_i \mid z_i, \theta) = \sum_{x=0}^{x_{\max}} p(y_i \mid x_i = x, z_i, \theta_y) \times p(x_i = x \mid z_i, \theta_x)</annotation></semantics></math></p>
<p>This analytic marginalization, implemented via numerically stable <code>log_sum_exp</code> operations, is crucial for computational efficiency. Treating discrete missing values as parameters to be sampled leads to poor mixing; marginalizing them out yields a smooth likelihood surface that Hamiltonian Monte Carlo explores efficiently.</p>
<p><strong>Prior Specification.</strong> We assign weakly informative priors throughout. Scale parameters (standard deviations, dispersion) receive exponential priors with rate 0.1, gently favoring smaller values while permitting large estimates when warranted. Fixed effects in the outcome model receive diffuse <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ùí©</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>0</mn><mo>,</mo><msup><mn>100</mn><mn>2</mn></msup><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathcal{N}(0, 100^2)</annotation></semantics></math> priors. Imputation model coefficients on the logit or log scale receive <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ùí©</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>0</mn><mo>,</mo><msup><mn>2.5</mn><mn>2</mn></msup><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathcal{N}(0, 2.5^2)</annotation></semantics></math> priors, providing mild regularization. The random effects correlation matrix receives an LKJ(2) prior, expressing modest skepticism toward extreme correlations.</p>
<p><strong>Computational Implementation.</strong> Our Stan models employ several optimizations. Random effects use a non-centered parameterization‚Äîwriting <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>b</mi><mi>i</mi></msub><mo>=</mo><mtext mathvariant="normal">diag</mtext><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>œÉ</mi><mi>b</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mi>L</mi><msub><mi>z</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">b_i = \text{diag}(\sigma_b) L z_i</annotation></semantics></math> where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>z</mi><mi>i</mi></msub><mo>‚àº</mo><mi>ùí©</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>0</mn><mo>,</mo><mi>I</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">z_i \sim \mathcal{N}(0, I)</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>L</mi><annotation encoding="application/x-tex">L</annotation></semantics></math> is a Cholesky factor‚Äîto avoid the pathological posterior geometries common in hierarchical models. Likelihood calculations are vectorized across observations, with precomputed index arrays (<code>id</code>, <code>pos</code>, <code>len</code>) enabling efficient subject-wise operations. Discrete covariate marginalization precomputes sufficient statistics to avoid redundant calculations across the summation.</p>
<p><strong>Diagnostics.</strong> We monitor standard MCMC diagnostics: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mi>R</mi><mo accent="true">ÃÇ</mo></mover><annotation encoding="application/x-tex">\hat{R}</annotation></semantics></math> statistics for convergence, effective sample sizes for mixing efficiency, divergent transitions for geometric pathologies, and energy Bayesian fraction of missing information (E-BFMI) for adequate exploration. Posterior predictive checks assess model fit for both outcomes and imputed covariates.</p>
</div>
<div class="section level3">
<h3 id="id_24-frequentist-comparator-ascertainment-corrected-maximum-likelihood">2.4 Frequentist Comparator: Ascertainment-Corrected Maximum Likelihood<a class="anchor" aria-label="anchor" href="#id_24-frequentist-comparator-ascertainment-corrected-maximum-likelihood"></a></h3>
<p>To contextualize Bayesian estimates, we also implement frequentist estimation via ascertainment-corrected maximum likelihood (ACML), available through <code><a href="reference/fit_acml_ods.html">fit_acml_ods()</a></code> for ODS designs.</p>
<p>The core idea is to adjust the likelihood for outcome-dependent selection. Under ODS, subjects in different strata have different selection probabilities, and ignoring this structure biases inference. ACML corrects by weighting each subject‚Äôs contribution inversely to their selection probability:</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>L</mi><mi>i</mi><mtext mathvariant="normal">ACML</mtext></msubsup><mrow><mo stretchy="true" form="prefix">(</mo><mi>Œ∏</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mfrac><mrow><mi>p</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo>‚à£</mo><msub><mi>x</mi><mi>i</mi></msub><mo>,</mo><msub><mi>z</mi><mi>i</mi></msub><mo>,</mo><msub><mi>Œ∏</mi><mi>y</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>√ó</mo><mi>p</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>‚à£</mo><msub><mi>z</mi><mi>i</mi></msub><mo>,</mo><msub><mi>Œ∏</mi><mi>x</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>i</mi><mo>‚àà</mo><mi>ùíÆ</mi><mo>‚à£</mo><msub><mi>y</mi><mi>i</mi></msub><mo>,</mo><msub><mi>z</mi><mi>i</mi></msub><mo>,</mo><mi>Œ∏</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow></mfrac></mrow><annotation encoding="application/x-tex">L_i^{\text{ACML}}(\theta) = \frac{p(y_i \mid x_i, z_i, \theta_y) \times p(x_i \mid z_i, \theta_x)}{P(i \in \mathcal{S} \mid y_i, z_i, \theta)}</annotation></semantics></math></p>
<p>The denominator‚Äîthe ascertainment probability‚Äîis computed from the known sampling design and stratum definitions. Standard errors come from robust sandwich estimation.</p>
<p>ACML provides a useful benchmark. It is faster than MCMC-based Bayesian inference and relies on different assumptions (correct likelihood specification rather than correct priors). Agreement between Bayesian and ACML estimates provides reassurance; disagreement prompts investigation.</p>
</div>
<div class="section level3">
<h3 id="id_25-computational-considerations">2.5 Computational Considerations<a class="anchor" aria-label="anchor" href="#id_25-computational-considerations"></a></h3>
<p>Practical application requires attention to computational details. Our Stan implementations in <code>src/stan/</code> are optimized for the specific likelihood structures arising in two-stage designs. Vectorized operations process all observations efficiently; subject-wise sufficient statistics avoid redundant calculations in the marginalization sums; non-centered random effects improve sampling geometry.</p>
<p>We recommend running multiple chains (typically four) to assess convergence and adjusting <code>adapt_delta</code> upward (to 0.95 or higher) if divergent transitions occur. For reproducibility across simulation studies, we use Cantor pairing to generate unique seeds for each simulation cell, ensuring results can be exactly replicated.</p>
<hr></div>
</div>
<div class="section level2">
<h2 id="id_3-simulation-study">3. Simulation Study<a class="anchor" aria-label="anchor" href="#id_3-simulation-study"></a></h2>
<p>We conducted extensive simulations to evaluate when and how much strategic sampling improves inference, and to characterize the operating characteristics of our Bayesian estimator.</p>
<div class="section level3">
<h3 id="id_31-goals">3.1 Goals<a class="anchor" aria-label="anchor" href="#id_31-goals"></a></h3>
<p>Our simulations address five questions:</p>
<ol style="list-style-type: decimal"><li>
<strong>Efficiency gains</strong>: How much do ODS and BDS improve precision for <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>Œ≤</mi><mi>x</mi></msub><annotation encoding="application/x-tex">\beta_x</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>Œ≤</mi><mrow><mi>t</mi><mi>x</mi></mrow></msub><annotation encoding="application/x-tex">\beta_{tx}</annotation></semantics></math> relative to SRS?</li>
<li>
<strong>BDS versus ODS</strong>: Under what conditions does BLUP-based sampling outperform OLS-based sampling?</li>
<li>
<strong>Coverage calibration</strong>: Do credible intervals achieve nominal coverage under correct model specification?</li>
<li>
<strong>Robustness</strong>: How does performance degrade when the imputation model is misspecified?</li>
<li>
<strong>Cost-effectiveness</strong>: How does precision scale with the fraction of subjects measured?</li>
</ol></div>
<div class="section level3">
<h3 id="id_32-data-generation">3.2 Data Generation<a class="anchor" aria-label="anchor" href="#id_32-data-generation"></a></h3>
<p>We generated data using <code><a href="reference/generate_data.html">generate_data()</a></code> across a factorial design spanning realistic scenarios:</p>
<p><strong>Sample sizes</strong>: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mo>‚àà</mo><mo stretchy="false" form="prefix">{</mo><mn>500</mn><mo>,</mo><mn>1000</mn><mo>,</mo><mn>2000</mn><mo stretchy="false" form="postfix">}</mo></mrow><annotation encoding="application/x-tex">N \in \{500, 1000, 2000\}</annotation></semantics></math>, representing moderate to large longitudinal cohorts.</p>
<p><strong>Time points</strong>: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi><mo>‚àà</mo><mo stretchy="false" form="prefix">{</mo><mn>3</mn><mo>,</mo><mn>5</mn><mo>,</mo><mn>8</mn><mo stretchy="false" form="postfix">}</mo></mrow><annotation encoding="application/x-tex">M \in \{3, 5, 8\}</annotation></semantics></math> observations per subject, spanning sparse to moderately dense follow-up.</p>
<p><strong>Random effects structure</strong>: Standard deviations <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>œÉ</mi><mrow><mi>b</mi><mn>0</mn></mrow></msub><mo>,</mo><msub><mi>œÉ</mi><mrow><mi>b</mi><mn>1</mn></mrow></msub><mo>‚àà</mo><mo stretchy="false" form="prefix">{</mo><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mn>4</mn><mo stretchy="false" form="postfix">}</mo></mrow><annotation encoding="application/x-tex">\sigma_{b0}, \sigma_{b1} \in \{1, 2, 4\}</annotation></semantics></math> and correlations <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>œÅ</mi><mo>‚àà</mo><mo stretchy="false" form="prefix">{</mo><mn>0</mn><mo>,</mo><mn>0.3</mn><mo>,</mo><mn>0.6</mn><mo stretchy="false" form="postfix">}</mo></mrow><annotation encoding="application/x-tex">\rho \in \{0, 0.3, 0.6\}</annotation></semantics></math>, capturing varying degrees of between-subject heterogeneity.</p>
<p><strong>Residual variation</strong>: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>œÉ</mi><mtext mathvariant="normal">main</mtext></msub><mo>‚àà</mo><mo stretchy="false" form="prefix">{</mo><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mn>4</mn><mo stretchy="false" form="postfix">}</mo></mrow><annotation encoding="application/x-tex">\sigma_{\text{main}} \in \{1, 2, 4\}</annotation></semantics></math>, representing low to high measurement noise.</p>
<p><strong>Effect sizes</strong>: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Œ≤</mi><mi>x</mi></msub><mo>‚àà</mo><mo stretchy="false" form="prefix">{</mo><mn>0</mn><mo>,</mo><mn>0.3</mn><mo>,</mo><mn>0.6</mn><mo>,</mo><mn>1.0</mn><mo stretchy="false" form="postfix">}</mo></mrow><annotation encoding="application/x-tex">\beta_x \in \{0, 0.3, 0.6, 1.0\}</annotation></semantics></math> for the main effect; <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Œ≤</mi><mrow><mi>t</mi><mi>x</mi></mrow></msub><mo>‚àà</mo><mo stretchy="false" form="prefix">{</mo><mn>0</mn><mo>,</mo><mn>0.2</mn><mo>,</mo><mn>0.4</mn><mo stretchy="false" form="postfix">}</mo></mrow><annotation encoding="application/x-tex">\beta_{tx} \in \{0, 0.2, 0.4\}</annotation></semantics></math> for the interaction. Null effects (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Œ≤</mi><mi>x</mi></msub><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\beta_x = 0</annotation></semantics></math> or <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Œ≤</mi><mrow><mi>t</mi><mi>x</mi></mrow></msub><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\beta_{tx} = 0</annotation></semantics></math>) allow assessment of type I error.</p>
<p><strong>Covariate types</strong>: Continuous (normal), binary (Bernoulli with <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo>‚àà</mo><mo stretchy="false" form="prefix">{</mo><mn>0.2</mn><mo>,</mo><mn>0.5</mn><mo stretchy="false" form="postfix">}</mo></mrow><annotation encoding="application/x-tex">p \in \{0.2, 0.5\}</annotation></semantics></math>), and count (negative binomial and beta-binomial with varying dispersion).</p>
<p><strong>Auxiliary covariate strength</strong>: The coefficient <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Œ≥</mi><annotation encoding="application/x-tex">\gamma</annotation></semantics></math> linking <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>z</mi><annotation encoding="application/x-tex">z</annotation></semantics></math> to <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math> varied across weak (0.2), moderate (0.6), and strong (1.0) associations. Stronger associations improve imputation accuracy.</p>
</div>
<div class="section level3">
<h3 id="id_33-design-factors">3.3 Design Factors<a class="anchor" aria-label="anchor" href="#id_33-design-factors"></a></h3>
<p>Within each data-generating scenario, we varied the sampling design:</p>
<p><strong>Sampling fractions</strong>: 5%, 10%, 20%, and 40% of subjects received expensive covariate measurement, spanning severely constrained to moderately generous budgets.</p>
<p><strong>Stratification</strong>: We used tertile-based strata with cutoffs at the 20th/80th or 25th/75th percentiles. Allocation to Low/Middle/High strata was either symmetric (33%/33%/33%) or tail-heavy (40%/20%/40%).</p>
<p><strong>Design types</strong>: SRS, ODS-intercept, ODS-slope, BDS-intercept, and BDS-slope.</p>
<p><strong>Misspecification scenarios</strong>: To stress-test robustness, we examined designs stratified on slopes when <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Œ≤</mi><mrow><mi>t</mi><mi>x</mi></mrow></msub><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\beta_{tx} = 0</annotation></semantics></math> (stratifying on an irrelevant quantity), and models fitting the wrong imputation family (e.g., normal imputation for count data).</p>
</div>
<div class="section level3">
<h3 id="id_34-estimators-and-metrics">3.4 Estimators and Metrics<a class="anchor" aria-label="anchor" href="#id_34-estimators-and-metrics"></a></h3>
<p>We compared five estimators:</p>
<ol style="list-style-type: decimal"><li>
<strong>Bayesian joint model</strong> with correct imputation specification</li>
<li>
<strong>Bayesian joint model</strong> with misspecified imputation</li>
<li>
<strong>ACML</strong> for ODS designs</li>
<li>
<strong>Naive complete-case analysis</strong> using only subjects with observed <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math></li>
<li>
<strong>Oracle</strong> with <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math> observed for all subjects (infeasible benchmark)</li>
</ol><p>For each estimator and scenario, we recorded:</p>
<ul><li>
<strong>Bias</strong>: Average deviation of point estimates from true values</li>
<li>
<strong>RMSE</strong>: Root mean squared error combining bias and variance</li>
<li>
<strong>Interval width</strong>: Average width of 95% credible or confidence intervals</li>
<li>
<strong>Coverage</strong>: Proportion of intervals containing the true value</li>
<li>
<strong>Power</strong>: Proportion of intervals excluding zero when the true effect is non-null</li>
<li>
<strong>Type I error</strong>: Proportion of intervals excluding zero when the true effect is null</li>
<li>
<strong>Relative efficiency</strong>: Variance ratio relative to SRS (for design comparisons) or oracle (for absolute efficiency)</li>
<li>
<strong>Imputation accuracy</strong>: Mean absolute error and calibration of imputed <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math> values</li>
<li>
<strong>Cost-adjusted precision</strong>: Precision per unit cost, accounting for Stage 2 measurement expenses</li>
<li>
<strong>Computational performance</strong>: Runtime, effective sample size per second, and divergent transition counts</li>
</ul><p>We ran at least 500 replications per scenario to ensure stable estimates of operating characteristics.</p>
</div>
<div class="section level3">
<h3 id="id_35-hypotheses">3.5 Hypotheses<a class="anchor" aria-label="anchor" href="#id_35-hypotheses"></a></h3>
<p>Based on theoretical considerations, we anticipated:</p>
<ul><li>ODS and BDS would reduce RMSE and narrow intervals for <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>Œ≤</mi><mi>x</mi></msub><annotation encoding="application/x-tex">\beta_x</annotation></semantics></math> relative to SRS, with gains largest at moderate effect sizes and 10‚Äì30% sampling fractions.</li>
<li>BDS would outperform ODS when trajectory estimates are unreliable: fewer time points, larger residual variance, or larger random effects variance (which increases BLUP shrinkage).</li>
<li>Coverage would be near nominal (95%) under correct imputation specification but could degrade under misspecification, particularly for discrete covariates.</li>
<li>Cost-adjusted efficiency would plateau beyond 20‚Äì30% sampling, with diminishing returns to additional measurements.</li>
</ul><hr></div>
</div>
<div class="section level2">
<h2 id="id_4-results">4. Results<a class="anchor" aria-label="anchor" href="#id_4-results"></a></h2>
<p><em>[This section will be populated with simulation results. The following structure and placeholder tables indicate the planned presentation.]</em></p>
<div class="section level3">
<h3 id="id_41-primary-comparison-sampling-strategy-effects">4.1 Primary Comparison: Sampling Strategy Effects<a class="anchor" aria-label="anchor" href="#id_41-primary-comparison-sampling-strategy-effects"></a></h3>
<p>Table 1 summarizes bias, RMSE, and coverage for <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>Œ≤</mi><mi>x</mi></msub><annotation encoding="application/x-tex">\beta_x</annotation></semantics></math> across sampling designs at 20% selection.</p>
<p><strong>Table 1.</strong> Performance for estimating <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>Œ≤</mi><mi>x</mi></msub><annotation encoding="application/x-tex">\beta_x</annotation></semantics></math> by sampling design (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mo>=</mo><mn>1000</mn></mrow><annotation encoding="application/x-tex">N = 1000</annotation></semantics></math>, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi><mo>=</mo><mn>5</mn></mrow><annotation encoding="application/x-tex">M = 5</annotation></semantics></math>, 20% selection, true <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Œ≤</mi><mi>x</mi></msub><mo>=</mo><mn>1.0</mn></mrow><annotation encoding="application/x-tex">\beta_x = 1.0</annotation></semantics></math>)</p>
<table class="table"><thead><tr class="header"><th>Design</th>
<th>Bias</th>
<th>RMSE</th>
<th>95% Coverage</th>
<th>Relative Efficiency</th>
</tr></thead><tbody><tr class="odd"><td>Oracle</td>
<td>[X.XXX]</td>
<td>[X.XXX]</td>
<td>[X.XX]</td>
<td>‚Äî</td>
</tr><tr class="even"><td>SRS</td>
<td>[X.XXX]</td>
<td>[X.XXX]</td>
<td>[X.XX]</td>
<td>1.00</td>
</tr><tr class="odd"><td>ODS-intercept</td>
<td>[X.XXX]</td>
<td>[X.XXX]</td>
<td>[X.XX]</td>
<td>[X.XX]</td>
</tr><tr class="even"><td>ODS-slope</td>
<td>[X.XXX]</td>
<td>[X.XXX]</td>
<td>[X.XX]</td>
<td>[X.XX]</td>
</tr><tr class="odd"><td>BDS-intercept</td>
<td>[X.XXX]</td>
<td>[X.XXX]</td>
<td>[X.XX]</td>
<td>[X.XX]</td>
</tr><tr class="even"><td>BDS-slope</td>
<td>[X.XXX]</td>
<td>[X.XXX]</td>
<td>[X.XX]</td>
<td>[X.XX]</td>
</tr></tbody></table><p><em>Key findings:</em> All methods showed negligible bias. ODS and BDS achieved [XX‚ÄìXX]% efficiency gains over SRS. BDS-intercept outperformed ODS-intercept by [XX]%, with the advantage concentrated in scenarios with noisy trajectory estimates.</p>
</div>
<div class="section level3">
<h3 id="id_42-cost-efficiency-tradeoff">4.2 Cost-Efficiency Tradeoff<a class="anchor" aria-label="anchor" href="#id_42-cost-efficiency-tradeoff"></a></h3>
<p>Figure 1 displays relative efficiency as a function of sampling fraction.</p>
<p><strong>[PLACEHOLDER: Figure 1]</strong> ‚Äî Efficiency curves showing precision relative to oracle across sampling fractions (5%‚Äì40%) for SRS, ODS, and BDS designs.</p>
<p><em>Key findings:</em> Strategic sampling provided the largest relative gains at low sampling fractions. At 10% selection, BDS achieved [XX]% of oracle efficiency versus [XX]% for SRS. Gains diminished above 30% selection, where all methods approached oracle performance.</p>
</div>
<div class="section level3">
<h3 id="id_43-when-does-bds-outperform-ods">4.3 When Does BDS Outperform ODS?<a class="anchor" aria-label="anchor" href="#id_43-when-does-bds-outperform-ods"></a></h3>
<p>Figure 2 examines the BDS advantage across data characteristics.</p>
<p><strong>[PLACEHOLDER: Figure 2]</strong> ‚Äî Panels showing BDS vs.¬†ODS relative efficiency by (A) number of time points, (B) residual variance, and (C) random effects variance.</p>
<p><em>Key findings:</em> BDS gains were largest with few time points (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi><mo>=</mo><mn>3</mn></mrow><annotation encoding="application/x-tex">M = 3</annotation></semantics></math>: [XX]% advantage), high residual variance (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>œÉ</mi><mo>=</mo><mn>4</mn></mrow><annotation encoding="application/x-tex">\sigma = 4</annotation></semantics></math>: [XX]% advantage), and large between-subject heterogeneity. With <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi><mo>=</mo><mn>8</mn></mrow><annotation encoding="application/x-tex">M = 8</annotation></semantics></math> and low noise, BDS and ODS performed similarly.</p>
</div>
<div class="section level3">
<h3 id="id_44-bayesian-vs-acml-estimation">4.4 Bayesian vs.¬†ACML Estimation<a class="anchor" aria-label="anchor" href="#id_44-bayesian-vs-acml-estimation"></a></h3>
<p>Table 2 compares estimation approaches.</p>
<p><strong>Table 2.</strong> Comparison of Bayesian and ACML estimation (ODS-intercept, 20% selection)</p>
<table class="table"><thead><tr class="header"><th>N</th>
<th>Method</th>
<th>Bias</th>
<th>Coverage</th>
<th>Mean SE</th>
<th>Time (sec)</th>
</tr></thead><tbody><tr class="odd"><td>500</td>
<td>Bayesian</td>
<td>[X.XXX]</td>
<td>[X.XX]</td>
<td>[X.XXX]</td>
<td>[XXX]</td>
</tr><tr class="even"><td>500</td>
<td>ACML</td>
<td>[X.XXX]</td>
<td>[X.XX]</td>
<td>[X.XXX]</td>
<td>[XX]</td>
</tr><tr class="odd"><td>1000</td>
<td>Bayesian</td>
<td>[X.XXX]</td>
<td>[X.XX]</td>
<td>[X.XXX]</td>
<td>[XXX]</td>
</tr><tr class="even"><td>1000</td>
<td>ACML</td>
<td>[X.XXX]</td>
<td>[X.XX]</td>
<td>[X.XXX]</td>
<td>[XX]</td>
</tr><tr class="odd"><td>2000</td>
<td>Bayesian</td>
<td>[X.XXX]</td>
<td>[X.XX]</td>
<td>[X.XXX]</td>
<td>[XXX]</td>
</tr><tr class="even"><td>2000</td>
<td>ACML</td>
<td>[X.XXX]</td>
<td>[X.XX]</td>
<td>[X.XXX]</td>
<td>[XX]</td>
</tr></tbody></table><p><em>Key findings:</em> Both methods were essentially unbiased. Bayesian coverage was closer to nominal in smaller samples. ACML was approximately [XX]√ó faster.</p>
</div>
<div class="section level3">
<h3 id="id_45-covariate-type-performance">4.5 Covariate Type Performance<a class="anchor" aria-label="anchor" href="#id_45-covariate-type-performance"></a></h3>
<p>Table 3 presents results across covariate distributions.</p>
<p><strong>Table 3.</strong> Performance by covariate type (BDS-intercept, 20% selection, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mo>=</mo><mn>1000</mn></mrow><annotation encoding="application/x-tex">N = 1000</annotation></semantics></math>)</p>
<table class="table"><thead><tr class="header"><th>Covariate Type</th>
<th>Bias</th>
<th>RMSE</th>
<th>Coverage</th>
<th>Efficiency vs.¬†SRS</th>
</tr></thead><tbody><tr class="odd"><td>Normal</td>
<td>[X.XXX]</td>
<td>[X.XXX]</td>
<td>[X.XX]</td>
<td>[X.XX]</td>
</tr><tr class="even"><td>Bernoulli</td>
<td>[X.XXX]</td>
<td>[X.XXX]</td>
<td>[X.XX]</td>
<td>[X.XX]</td>
</tr><tr class="odd"><td>Negative Binomial</td>
<td>[X.XXX]</td>
<td>[X.XXX]</td>
<td>[X.XX]</td>
<td>[X.XX]</td>
</tr><tr class="even"><td>Beta-Binomial</td>
<td>[X.XXX]</td>
<td>[X.XXX]</td>
<td>[X.XX]</td>
<td>[X.XX]</td>
</tr></tbody></table><p><em>Key findings:</em> All covariate types achieved valid coverage. Efficiency gains from BDS were comparable across types, with slightly attenuated gains for binary covariates due to discreteness.</p>
</div>
<div class="section level3">
<h3 id="id_46-robustness-to-misspecification">4.6 Robustness to Misspecification<a class="anchor" aria-label="anchor" href="#id_46-robustness-to-misspecification"></a></h3>
<p>Table 4 examines imputation model misspecification.</p>
<p><strong>Table 4.</strong> Effect of imputation model misspecification</p>
<table class="table"><thead><tr class="header"><th>True Distribution</th>
<th>Fitted Model</th>
<th>Bias</th>
<th>Coverage</th>
</tr></thead><tbody><tr class="odd"><td>Normal</td>
<td>Normal</td>
<td>[X.XXX]</td>
<td>[X.XX]</td>
</tr><tr class="even"><td>Mixture Normal</td>
<td>Normal</td>
<td>[X.XXX]</td>
<td>[X.XX]</td>
</tr><tr class="odd"><td>Negative Binomial</td>
<td>Normal</td>
<td>[X.XXX]</td>
<td>[X.XX]</td>
</tr><tr class="even"><td>Zero-inflated</td>
<td>Negative Binomial</td>
<td>[X.XXX]</td>
<td>[X.XX]</td>
</tr></tbody></table><p><em>Key findings:</em> Moderate misspecification (fitting normal to mixture normal) had limited impact. Severe misspecification (fitting normal to counts) degraded coverage by [XX]%, though bias remained modest.</p>
</div>
<div class="section level3">
<h3 id="id_47-summary-of-simulation-findings">4.7 Summary of Simulation Findings<a class="anchor" aria-label="anchor" href="#id_47-summary-of-simulation-findings"></a></h3>
<p>The simulations support four main conclusions:</p>
<ol style="list-style-type: decimal"><li><p><strong>Strategic sampling improves efficiency</strong>: ODS and BDS reduce RMSE for covariate effects by [XX‚ÄìXX]% relative to SRS, with benefits concentrated at 10‚Äì30% sampling fractions.</p></li>
<li><p><strong>BDS outperforms ODS when trajectories are noisy</strong>: The BLUP-based approach provides additional gains when individual trajectory estimates are unreliable.</p></li>
<li><p><strong>Inference is well-calibrated</strong>: Coverage remains near nominal under correct specification, with Bayesian methods showing advantages in small samples.</p></li>
<li><p><strong>Robustness is moderate</strong>: Mild imputation misspecification has limited impact, but severe misspecification warrants sensitivity analysis.</p></li>
</ol><hr></div>
</div>
<div class="section level2">
<h2 id="id_5-case-study">5. Case Study<a class="anchor" aria-label="anchor" href="#id_5-case-study"></a></h2>
<p><em>[This section will present an applied example demonstrating the complete workflow.]</em></p>
<div class="section level3">
<h3 id="id_51-study-description">5.1 Study Description<a class="anchor" aria-label="anchor" href="#id_51-study-description"></a></h3>
<p>[Describe the motivating application: scientific question, cohort characteristics, expensive covariate, available longitudinal outcomes, and auxiliary covariates.]</p>
</div>
<div class="section level3">
<h3 id="id_52-design-construction">5.2 Design Construction<a class="anchor" aria-label="anchor" href="#id_52-design-construction"></a></h3>
<p>[Apply <code><a href="reference/ods_design.html">ods_design()</a></code> or <code><a href="reference/bds_design.html">bds_design()</a></code> to construct the Stage 2 sample. Report stratification variable, cutoffs, stratum sizes, and achieved sampling fraction.]</p>
</div>
<div class="section level3">
<h3 id="id_53-model-fitting">5.3 Model Fitting<a class="anchor" aria-label="anchor" href="#id_53-model-fitting"></a></h3>
<p>[Fit the Bayesian joint model via <code><a href="reference/fit_stan_model.html">fit_stan_model()</a></code> with appropriate covariate type. If ODS, also fit ACML for comparison. Report convergence diagnostics.]</p>
</div>
<div class="section level3">
<h3 id="id_54-results">5.4 Results<a class="anchor" aria-label="anchor" href="#id_54-results"></a></h3>
<p>[Present effect estimates with credible intervals. Include posterior predictive checks for outcomes and imputed covariates. Visualize selection: density plots of stratification variable with cutoff lines, trajectory plots for sampled vs.¬†unsampled subjects.]</p>
</div>
<div class="section level3">
<h3 id="id_55-interpretation">5.5 Interpretation<a class="anchor" aria-label="anchor" href="#id_55-interpretation"></a></h3>
<p>[Discuss scientific implications of the estimated covariate effects. Address practical considerations: cost savings achieved, limitations of the analysis, recommendations for future studies.]</p>
<hr></div>
</div>
<div class="section level2">
<h2 id="id_6-discussion">6. Discussion<a class="anchor" aria-label="anchor" href="#id_6-discussion"></a></h2>
<p>Two-stage designs offer a principled solution to the common challenge of expensive covariates in longitudinal research. By measuring expensive variables on strategically selected subjects and properly modeling the resulting missing data, researchers can achieve substantial efficiency gains while maintaining valid inference.</p>
<div class="section level3">
<h3 id="key-findings">Key Findings<a class="anchor" aria-label="anchor" href="#key-findings"></a></h3>
<p>Our Bayesian framework, combining mixed-effects outcome models with flexible covariate imputation, provides well-calibrated uncertainty quantification across diverse scenarios. Tail-focused ODS and BDS strategies consistently outperform simple random sampling, with efficiency gains of [XX‚ÄìXX]% at typical sampling fractions. The magnitude of these gains depends on effect size, sampling fraction, and data characteristics, but meaningful improvements are achievable across realistic settings.</p>
<p>BLUP-dependent sampling offers additional advantages when individual trajectory estimates are unreliable. By shrinking noisy estimates toward the population mean, BDS avoids the misclassification of average subjects as extreme that can plague OLS-based approaches. The benefits are largest with few time points, high measurement error, or substantial between-subject heterogeneity‚Äîprecisely the conditions where efficient design matters most.</p>
</div>
<div class="section level3">
<h3 id="practical-recommendations">Practical Recommendations<a class="anchor" aria-label="anchor" href="#practical-recommendations"></a></h3>
<p>For applied researchers, we offer several recommendations:</p>
<p><strong>Choosing between ODS and BDS</strong>: If subjects contribute many precise observations (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi><mo>‚â•</mo><mn>5</mn></mrow><annotation encoding="application/x-tex">M \geq 5</annotation></semantics></math>, low <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>œÉ</mi><annotation encoding="application/x-tex">\sigma</annotation></semantics></math>), OLS-based ODS is simpler and performs nearly as well as BDS. With sparse or noisy data, BDS provides meaningful additional efficiency.</p>
<p><strong>Selecting the sampling fraction</strong>: A 20% fraction often balances cost and precision well. Below 10%, even strategic sampling yields limited information; above 30%, gains from strategic selection diminish relative to simpler designs.</p>
<p><strong>Stratification variable</strong>: Target intercepts when the main effect <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>Œ≤</mi><mi>x</mi></msub><annotation encoding="application/x-tex">\beta_x</annotation></semantics></math> is primary; target slopes when the interaction <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>Œ≤</mi><mrow><mi>t</mi><mi>x</mi></mrow></msub><annotation encoding="application/x-tex">\beta_{tx}</annotation></semantics></math> matters most.</p>
<p><strong>Imputation model</strong>: Match the distributional family to the covariate type. When uncertain, sensitivity analyses with alternative specifications are advisable.</p>
</div>
<div class="section level3">
<h3 id="limitations">Limitations<a class="anchor" aria-label="anchor" href="#limitations"></a></h3>
<p>Several limitations warrant acknowledgment. Our methods assume missingness is at random conditional on observed data‚Äîthat selection depends only on outcomes and auxiliary covariates, not on the unobserved expensive covariate itself. Violations could occur if subjects with extreme covariate values are differentially difficult to recruit. The imputation model must be at least approximately correctly specified; severe misspecification degrades coverage. We assume linear trajectories; non-linear patterns require model extensions. The expensive covariate is time-invariant (measured once at baseline); time-varying expensive measurements require additional methodology. Finally, computation scales with sample size despite our optimizations.</p>
</div>
<div class="section level3">
<h3 id="future-directions">Future Directions<a class="anchor" aria-label="anchor" href="#future-directions"></a></h3>
<p>Several extensions merit investigation. Time-varying expensive covariates‚Äîwhere the costly measurement is repeated at selected occasions‚Äîarise naturally in biomarker monitoring. Non-linear trajectories, modeled through splines or Gaussian processes, would accommodate more complex longitudinal patterns. Informative dropout, where missingness of outcomes depends on unobserved health status, is common in aging cohorts. Adaptive designs that update selection probabilities sequentially as data accumulate could further improve efficiency. Finally, scalable computation through variational approximations or GPU acceleration would extend applicability to very large cohorts.</p>
<hr></div>
</div>
<div class="section level2">
<h2 id="id_7-software-and-reproducibility">7. Software and Reproducibility<a class="anchor" aria-label="anchor" href="#id_7-software-and-reproducibility"></a></h2>
<p>All methods are implemented in the R package <code>bayes2stage</code>, available at [GitHub URL]. The package provides:</p>
<ul><li>
<strong>Design functions</strong>: <code><a href="reference/srs_design.html">srs_design()</a></code>, <code><a href="reference/ods_design.html">ods_design()</a></code>, <code><a href="reference/bds_design.html">bds_design()</a></code> for constructing Stage 2 samples</li>
<li>
<strong>Estimation</strong>: <code><a href="reference/fit_stan_model.html">fit_stan_model()</a></code> for Bayesian inference; <code><a href="reference/fit_acml_ods.html">fit_acml_ods()</a></code> for frequentist ACML</li>
<li>
<strong>Simulation</strong>: <code><a href="reference/generate_data.html">generate_data()</a></code> for generating synthetic longitudinal data</li>
<li>
<strong>Diagnostics</strong>: Functions for MCMC assessment, posterior summaries, and visualization</li>
</ul><p>Stan models reside in <code>src/stan/</code>, with separate implementations for each covariate type. The package vignette (<code>vignettes/bayes2stage.qmd</code>) provides a tutorial walking through the complete workflow.</p>
<p>For reproducibility, simulation scripts include fixed seeds using Cantor pairing for unique per-cell randomization. We recommend using <code>renv</code> or containerization to ensure consistent package versions.</p>
<hr></div>
<div class="section level2">
<h2 id="id_8-acknowledgments">8. Acknowledgments<a class="anchor" aria-label="anchor" href="#id_8-acknowledgments"></a></h2>
<p>[Insert funding sources, collaborator acknowledgments, and data access statements.]</p>
<hr></div>
<div class="section level2">
<h2 id="id_9-references">9. References<a class="anchor" aria-label="anchor" href="#id_9-references"></a></h2>
<p>Betancourt, M. (2017). A conceptual introduction to Hamiltonian Monte Carlo. <em>arXiv preprint arXiv:1701.02434</em>.</p>
<p>Breslow, N. E., and Chatterjee, N. (1999). Design and analysis of two-phase studies with binary outcome applied to Wilms tumour prognosis. <em>Journal of the Royal Statistical Society: Series C</em>, 48, 457‚Äì468.</p>
<p>Carpenter, B., Gelman, A., Hoffman, M. D., Lee, D., Goodrich, B., Betancourt, M., Brubaker, M., Guo, J., Li, P., and Riddell, A. (2017). Stan: A probabilistic programming language. <em>Journal of Statistical Software</em>, 76(1), 1‚Äì32.</p>
<p>Little, R. J., and Rubin, D. B. (2019). <em>Statistical Analysis with Missing Data</em> (3rd ed.). Wiley.</p>
<p>Schildcrout, J. S., and Rathouz, P. J. (2010). Longitudinal studies of binary response data following case-control and stratified case-control sampling: Design and analysis. <em>Biometrics</em>, 66, 365‚Äì373.</p>
<p>Weaver, M. A., and Zhou, H. (2005). An estimated likelihood method for continuous outcome regression models with outcome-dependent sampling. <em>Journal of the American Statistical Association</em>, 100, 459‚Äì469.</p>
<p>Zhou, H., Weaver, M. A., Qin, J., Longnecker, M. P., and Wang, M. C. (2002). A semiparametric empirical likelihood method for data from an outcome-dependent sampling scheme with a continuous outcome. <em>Biometrics</em>, 58, 413‚Äì421.</p>
<hr></div>
<div class="section level2">
<h2 id="appendix-a-stan-model-implementation-details">Appendix A: Stan Model Implementation Details<a class="anchor" aria-label="anchor" href="#appendix-a-stan-model-implementation-details"></a></h2>
<p><em>[Technical details of the Stan implementations, including the non-centered parameterization, marginalization strategy for discrete covariates, and vectorization approach.]</em></p>
</div>
<div class="section level2">
<h2 id="appendix-b-additional-simulation-results">Appendix B: Additional Simulation Results<a class="anchor" aria-label="anchor" href="#appendix-b-additional-simulation-results"></a></h2>
<p><em>[Extended tables presenting results for all parameter combinations, including <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>Œ≤</mi><mi>z</mi></msub><annotation encoding="application/x-tex">\beta_z</annotation></semantics></math>, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>Œ≤</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\beta_t</annotation></semantics></math>, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>Œ≤</mi><mrow><mi>t</mi><mi>x</mi></mrow></msub><annotation encoding="application/x-tex">\beta_{tx}</annotation></semantics></math>, and variance components.]</em></p>
</div>
<div class="section level2">
<h2 id="appendix-c-sensitivity-analyses">Appendix C: Sensitivity Analyses<a class="anchor" aria-label="anchor" href="#appendix-c-sensitivity-analyses"></a></h2>
<p><em>[Results from alternative prior specifications, varying <code>adapt_delta</code>, and different stratification cutoffs.]</em></p>
<hr><p><em>Manuscript prepared with R Markdown. Reproducible code is available at [repository URL].</em></p>
</div>
</div>

  </main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside></div>


    <footer><div class="pkgdown-footer-left">
  <p>Developed by Maximilian Rohde.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.2.0.</p>
</div>

    </footer></div>





  </body></html>

